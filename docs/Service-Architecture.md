# ğŸ›ï¸ Service Architecture

**Version:** 4.0 (Modular Monolith)  
**Last Updated:** November 14, 2025  
**Status:** Active Development

---

## ğŸ“‹ Table of Contents

1. [Architecture Evolution](#architecture-evolution)
2. [Modular Monolith Design](#modular-monolith-design)
3. [Service Modules](#service-modules)
4. [Module Communication](#module-communication)
5. [Directory Structure](#directory-structure)
6. [Deployment Model](#deployment-model)
7. [Migration Path](#migration-path)

---

## ğŸ”„ Architecture Evolution

### **v1-3: 19 Microservices (Deprecated)**

```
SMB Gateway â†’ Goal Service â†’ Coach Service â†’ Optimization Engine
                           â†“
           Forecasting â† Evidence â† Connector Service
                           â†“
           Analytics â† Notification â† ... (11 more services)
```

**Problems:**

- âŒ **High operational complexity** (19 deployments, 19 configs, 19 log streams)
- âŒ **Network overhead** (inter-service REST calls add 50-200ms latency)
- âŒ **Difficult debugging** (distributed tracing required, hard to reproduce issues)
- âŒ **Slow development** (coordinated releases, breaking changes across services)

---

### **v4.0: Modular Monolith (Current)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastAPI Monolith (Single Process)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GoalService â”‚ CoachService â”‚ OptimizerService â”‚
â”‚ ForecastService â”‚ EvidenceService â”‚ etc.      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ (Direct function calls)
      PostgreSQL (Single Database)
```

**Benefits:**

- âœ… **80% reduction in ops complexity** (1 deployment, 1 log stream, 1 config)
- âœ… **Zero network overhead** (in-memory function calls, <1ms)
- âœ… **Easy debugging** (single process, standard Python debugger works)
- âœ… **Fast iteration** (change multiple modules in one commit)
- âœ… **Cost-effective** (single VM scales to 5000 SMBs)

---

## ğŸ§© Modular Monolith Design

### **Design Principles**

1. **Domain-Driven Design (DDD)**
   - Each module owns a bounded context (Goals, Coach, Optimization, etc.)
   - Clear interfaces between modules (dependency injection)

2. **Loose Coupling**
   - Modules communicate via interfaces (not direct imports)
   - Changes to one module don't break others

3. **High Cohesion**
   - Related functionality grouped together
   - Each module has single responsibility

4. **Easy to Extract**
   - If scale demands, modules can become microservices later
   - Minimal code changes (interface â†’ REST API)

---

## ğŸ› ï¸ Service Modules

### **1. GoalService**

**Responsibility:** SMART goal lifecycle management

**Core Functions:**

- `create_goal(tenant_id, user_id, goal_data)` â†’ Validate SMART criteria, store in DB
- `update_goal_progress(goal_id, metrics)` â†’ Track progress, trigger alerts
- `evaluate_goal(goal_id)` â†’ Assess achievement, generate insights
- `suggest_goals(tenant_id, business_context)` â†’ AI-powered goal recommendations

**Tech Stack:** SQLAlchemy ORM, Pydantic validation

**Database Tables:**

- `smart_goals` (goal definitions, progress, status)
- `goal_milestones` (checkpoints along the way)

---

### **2. CoachService (Multi-Agent)**

**Responsibility:** Conversational AI coach

**Core Functions:**

- `ask_coach(query, context)` â†’ Route to appropriate agent (Goal Planner, Forecaster, etc.)
- `stream_response(query)` â†’ Server-Sent Events for real-time output
- `retrieve_context(query)` â†’ RAG using pgvector for similar conversations

**Tech Stack:** LangGraph (state machines), LangChain (tools), Ollama/OpenAI (LLMs)

**Agents:**

1. **Goal Planner** â†’ Decompose user intent into SMART goals
2. **Evidence Analyzer** â†’ Explain why metrics changed (causal inference)
3. **Forecaster** â†’ Predict future outcomes
4. **Optimizer** â†’ Recommend optimal actions

**Database Tables:**

- `coaching_sessions` (conversation history + embeddings)
- `agent_executions` (trace which agents ran, what they returned)

---

### **3. OptimizerService**

**Responsibility:** Mathematical optimization for operations

**Core Functions:**

- `optimize_inventory(tenant_id, constraints)` â†’ Minimize holding costs + stockouts
- `optimize_staffing(tenant_id, demand_forecast)` â†’ Minimize labor costs, meet service levels
- `optimize_budget(tenant_id, channels)` â†’ Maximize ROI across marketing channels

**Tech Stack:** OR-Tools (Google), PuLP (Python LP)

**Techniques:**

- Linear Programming (LP) for continuous decisions
- Mixed-Integer Programming (MILP) for discrete choices (hire 3 staff, not 3.5)
- Constraint Programming (CP) for complex scheduling

**Database Tables:**

- `optimization_runs` (inputs, outputs, solver metrics)
- `optimization_constraints` (business rules per tenant)

---

### **4. ForecasterService**

**Responsibility:** Time-series predictions with uncertainty

**Core Functions:**

- `forecast_metric(metric_name, horizon, confidence_level)` â†’ Return point estimates + intervals
- `ensemble_forecast(metric_name)` â†’ Combine ARIMA + Prophet + XGBoost
- `detect_anomalies(metric_name)` â†’ Flag unusual values

**Tech Stack:** statsmodels (ARIMA), Prophet (Meta), XGBoost, scikit-learn

**Models:**

1. **Auto-ARIMA** â†’ Automatic parameter selection for stationary series
2. **Prophet** â†’ Handles seasonality, holidays, trend changes
3. **XGBoost** â†’ Feature-based (lagged values, exogenous variables)
4. **Ensemble** â†’ Weighted average (typically 70% accuracy improvement)

**Database Tables:**

- `forecasts` (predictions with confidence intervals)
- `forecast_models` (trained model metadata, performance metrics)

---

### **5. EvidenceService**

**Responsibility:** Causal inference and root cause analysis

**Core Functions:**

- `explain_change(metric_name, time_range)` â†’ Identify causal factors
- `simulate_intervention(action, expected_impact)` â†’ "What if we hire 2 more staff?"
- `build_causal_graph(tenant_id)` â†’ Learn DAG from historical data

**Tech Stack:** DoWhy (Microsoft), pgmpy (Bayesian networks), CausalNex

**Methods:**

1. **Granger Causality** â†’ Detect time-lagged relationships
2. **Propensity Score Matching** â†’ Estimate causal effects from observational data
3. **Counterfactual Analysis** â†’ Compare actual vs. hypothetical outcomes

**Database Tables:**

- `evidence_graph` (causal DAG as JSONB or Apache AGE graph)
- `causal_estimates` (effect sizes, confidence intervals, p-values)

---

### **6. ConnectorService**

**Responsibility:** Data ingestion from external sources

**Core Functions:**

- `connect_datasource(tenant_id, source_type, credentials)` â†’ OAuth2 flow, store tokens
- `sync_data(connector_id)` â†’ Fetch new/updated records
- `transform_data(raw_data, mapping)` â†’ Normalize to internal schema

**Tech Stack:** Requests (HTTP client), OAuth2 libraries, Pandas (data transforms)

**Supported Connectors:**

- **Accounting:** QuickBooks, Xero, FreshBooks
- **E-commerce:** Shopify, WooCommerce, BigCommerce
- **CRM:** Salesforce, HubSpot, Pipedrive
- **Payments:** Stripe, Square, PayPal
- **Custom:** CSV/Excel upload, REST API webhooks

**Database Tables:**

- `data_sources` (connector configs, credentials, sync status)
- `connector_sync_logs` (history of syncs, errors)
- `raw_data_staging` (temporary storage before transformation)

---

### **7. AnalyticsService**

**Responsibility:** Query builder and metric aggregation

**Core Functions:**

- `query_metrics(tenant_id, filters, aggregations)` â†’ Flexible SQL generation
- `create_dashboard(tenant_id, widgets)` â†’ Save custom dashboard configs
- `export_data(query, format)` â†’ CSV, Excel, JSON exports

**Tech Stack:** SQLAlchemy (query building), Pandas (aggregations)

**Database Tables:**

- `dashboards` (user-defined dashboard layouts)
- `saved_queries` (frequently used metric queries)

---

### **8. NotificationService**

**Responsibility:** Multi-channel alerts

**Core Functions:**

- `send_notification(tenant_id, user_id, message, channel)` â†’ Email, Slack, in-app
- `schedule_notification(trigger, message)` â†’ Weekly goal reminders
- `send_alert(condition, severity)` â†’ Metric threshold breaches

**Tech Stack:** SMTP (email), Slack SDK, WebSockets (in-app)

**Channels:**

- **Email:** SMTP relay (SendGrid, Mailgun, or self-hosted Postfix)
- **Slack:** Webhook or OAuth app
- **In-App:** WebSocket push to frontend
- **Webhooks:** Custom HTTP callbacks

**Database Tables:**

- `notification_preferences` (user settings per channel)
- `notification_queue` (pending notifications, retry logic)

---

## ğŸ”— Module Communication

### **1. Direct Function Calls (Primary)**

```python
# Example: Coach calls Optimizer
from services.optimizer_service import OptimizerService
from services.coach_service import CoachService

class CoachService:
    def __init__(self, optimizer: OptimizerService):
        self.optimizer = optimizer  # Dependency injection
    
    async def handle_query(self, query: str):
        if "optimize inventory" in query.lower():
            result = await self.optimizer.optimize_inventory(...)
            return self.format_response(result)
```

**Benefits:**

- âš¡ **Fast:** In-memory calls (<1ms overhead)
- ğŸ› **Easy to debug:** Standard Python debugger works
- ğŸ”’ **Type-safe:** mypy catches errors at compile time

---

### **2. Database as Integration Point (Secondary)**

For asynchronous workflows:

```python
# Module A writes to database
await db.execute("INSERT INTO tasks (type, payload) VALUES ('forecast', $1)", payload)

# Module B listens via PostgreSQL LISTEN/NOTIFY
await db.execute("LISTEN task_queue")
async for notification in db.notifications():
    if notification.channel == "task_queue":
        process_task(notification.payload)
```

---

### **3. Event Bus (Optional for Complex Flows)**

For decoupled pub/sub:

```python
from events import EventBus

# Publisher
await event_bus.publish("goal.completed", {"goal_id": "123", "outcome": "achieved"})

# Subscriber (in another module)
@event_bus.subscribe("goal.completed")
async def on_goal_completed(event):
    await notification_service.send_congratulations(event["goal_id"])
```

---

## ğŸ“ Directory Structure

```
backend/
â”œâ”€â”€ main.py                    # FastAPI app entry point
â”œâ”€â”€ config.py                  # Environment variables, settings
â”œâ”€â”€ dependencies.py            # Dependency injection setup
â”‚
â”œâ”€â”€ routes/                    # API endpoints (thin layer)
â”‚   â”œâ”€â”€ auth.py               # POST /auth/login, /auth/logout
â”‚   â”œâ”€â”€ goals.py              # CRUD for /goals/*
â”‚   â”œâ”€â”€ coach.py              # POST /coach/ask (streaming)
â”‚   â”œâ”€â”€ optimize.py           # POST /optimize/{type}
â”‚   â”œâ”€â”€ forecast.py           # GET /forecast/{metric}
â”‚   â””â”€â”€ evidence.py           # GET /evidence/explain
â”‚
â”œâ”€â”€ services/                  # Business logic (thick layer)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ goal_service.py       # GoalService class
â”‚   â”œâ”€â”€ coach_service.py      # CoachService class
â”‚   â”œâ”€â”€ optimizer_service.py  # OptimizerService class
â”‚   â”œâ”€â”€ forecaster_service.py # ForecasterService class
â”‚   â”œâ”€â”€ evidence_service.py   # EvidenceService class
â”‚   â”œâ”€â”€ connector_service.py  # ConnectorService class
â”‚   â”œâ”€â”€ analytics_service.py  # AnalyticsService class
â”‚   â””â”€â”€ notification_service.py # NotificationService class
â”‚
â”œâ”€â”€ agents/                    # LangGraph agent definitions
â”‚   â”œâ”€â”€ goal_planner.py       # Goal decomposition agent
â”‚   â”œâ”€â”€ evidence_analyzer.py  # Causal explanation agent
â”‚   â”œâ”€â”€ forecaster_agent.py   # Prediction agent
â”‚   â””â”€â”€ optimizer_agent.py    # Recommendation agent
â”‚
â”œâ”€â”€ models/                    # SQLAlchemy ORM models
â”‚   â”œâ”€â”€ tenant.py
â”‚   â”œâ”€â”€ user.py
â”‚   â”œâ”€â”€ goal.py
â”‚   â”œâ”€â”€ metric.py
â”‚   â”œâ”€â”€ session.py
â”‚   â””â”€â”€ connector.py
â”‚
â”œâ”€â”€ schemas/                   # Pydantic request/response models
â”‚   â”œâ”€â”€ goal_schemas.py
â”‚   â”œâ”€â”€ coach_schemas.py
â”‚   â””â”€â”€ optimization_schemas.py
â”‚
â”œâ”€â”€ utils/                     # Helpers
â”‚   â”œâ”€â”€ auth.py               # JWT token management
â”‚   â”œâ”€â”€ logging.py            # Structured logging
â”‚   â”œâ”€â”€ metrics.py            # Prometheus instrumentation
â”‚   â””â”€â”€ cache.py              # Redis caching
â”‚
â””â”€â”€ tests/                     # Unit + integration tests
    â”œâ”€â”€ unit/
    â”‚   â”œâ”€â”€ test_goal_service.py
    â”‚   â””â”€â”€ test_coach_service.py
    â””â”€â”€ integration/
        â””â”€â”€ test_coach_optimization_flow.py
```

---

## ğŸš€ Deployment Model

### **Development (Docker Compose)**

```yaml
version: '3.8'
services:
  postgres:
    image: timescale/timescaledb-ha:pg16
    ports: ["5432:5432"]
  
  backend:
    build: ./backend
    command: uvicorn main:app --reload --host 0.0.0.0 --port 8000
    ports: ["8000:8000"]
    depends_on: [postgres]
  
  frontend:
    build: ./apps/smb
    command: npm run dev
    ports: ["3000:3000"]
```

**Start:** `docker-compose up -d`

---

### **Production (Single VM, <5000 SMBs)**

```bash
# 1. Build backend
cd backend && docker build -t dyocense-backend .

# 2. Run with systemd
sudo systemctl start dyocense-backend

# 3. Reverse proxy (Nginx)
sudo nginx -s reload  # Routes traffic to backend:8000
```

**Vertical Scaling:** 16 vCPU, 64GB RAM â†’ handles 5000 SMBs

---

### **Scale (Kubernetes, >5000 SMBs)**

```yaml
# kubernetes/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dyocense-backend
spec:
  replicas: 5  # Horizontal scaling
  template:
    spec:
      containers:
      - name: backend
        image: dyocense-backend:latest
        resources:
          requests: {cpu: "2", memory: "4Gi"}
```

**Database:** PostgreSQL with read replicas (scale reads)

---

## ğŸ”„ Migration Path

### **From Microservices to Monolith**

**Phase 1: Consolidate Shared Libraries**

1. Extract common code (auth, logging, metrics) into `utils/`
2. Remove duplicated code across old services

**Phase 2: Merge Services into Modules**

1. Copy `services/goal_service/main.py` â†’ `backend/services/goal_service.py`
2. Convert REST APIs to function calls
3. Update imports (remove network clients)

**Phase 3: Unify Database Access**

1. Merge schemas into single Alembic migration directory
2. Apply Row-Level Security (RLS) policies
3. Remove per-service database users (use single app user)

**Phase 4: Update Deployment**

1. Replace 19 Docker containers with 1
2. Simplify CI/CD (single build/deploy pipeline)
3. Update monitoring (1 target instead of 19)

**Phase 5: Test & Validate**

1. Run integration tests (verify no regressions)
2. Load test (ensure performance meets SLAs)
3. Beta deploy (10 SMBs for 1 week)

---

## ğŸ¯ Next Steps

1. **Review [Design Document](./Design-Document.md)** for overall architecture
2. **Review [Data Architecture](./Data-Architecture.md)** for PostgreSQL schema
3. **Start Phase 0** of [Implementation Roadmap](./Implementation-Roadmap.md)

**Ready to build! ğŸš€**
